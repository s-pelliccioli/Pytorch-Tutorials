{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nfrom typing import Tuple\n\nimport torch\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import dataset\n\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n                 nlayers: int, dropout: float = 0.5):\n        super().__init__()\n        self.model_type = 'Transformer'\n        self.pos_encoder = PositionalEncoding(d_model, dropout)\n        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n        self.encoder = nn.Embedding(ntoken, d_model)\n        self.d_model = d_model\n        self.decoder = nn.Linear(d_model, ntoken)\n\n        self.init_weights()\n\n    def init_weights(self) -> None:\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            src: Tensor, shape [seq_len, batch_size]\n            src_mask: Tensor, shape [seq_len, seq_len]\n\n        Returns:\n            output Tensor of shape [seq_len, batch_size, ntoken]\n        \"\"\"\n        src = self.encoder(src) * math.sqrt(self.d_model)\n        src = self.pos_encoder(src)\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output\n\n\ndef generate_square_subsequent_mask(sz: int) -> Tensor:\n    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-24T21:26:15.809616Z","iopub.execute_input":"2022-02-24T21:26:15.810082Z","iopub.status.idle":"2022-02-24T21:26:17.556625Z","shell.execute_reply.started":"2022-02-24T21:26:15.809991Z","shell.execute_reply":"2022-02-24T21:26:17.555682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: Tensor) -> Tensor:\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:26:45.306798Z","iopub.execute_input":"2022-02-24T21:26:45.307184Z","iopub.status.idle":"2022-02-24T21:26:45.317654Z","shell.execute_reply.started":"2022-02-24T21:26:45.307138Z","shell.execute_reply":"2022-02-24T21:26:45.316566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext.datasets import WikiText2\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntrain_iter = WikiText2(split='train')\ntokenizer = get_tokenizer('basic_english')\nvocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\nvocab.set_default_index(vocab['<unk>'])\n\ndef data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n\n# train_iter was \"consumed\" by the process of building the vocab,\n# so we have to create it again\ntrain_iter, val_iter, test_iter = WikiText2()\ntrain_data = data_process(train_iter)\nval_data = data_process(val_iter)\ntest_data = data_process(test_iter)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndef batchify(data: Tensor, bsz: int) -> Tensor:\n    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n    that wouldn't cleanly fit.\n\n    Args:\n        data: Tensor, shape [N]\n        bsz: int, batch size\n\n    Returns:\n        Tensor of shape [N // bsz, bsz]\n    \"\"\"\n    seq_len = data.size(0) // bsz\n    data = data[:seq_len * bsz]\n    data = data.view(bsz, seq_len).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\ntrain_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\nval_data = batchify(val_data, eval_batch_size)\ntest_data = batchify(test_data, eval_batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:27:49.641321Z","iopub.execute_input":"2022-02-24T21:27:49.642346Z","iopub.status.idle":"2022-02-24T21:27:59.954084Z","shell.execute_reply.started":"2022-02-24T21:27:49.642282Z","shell.execute_reply":"2022-02-24T21:27:59.952996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bptt = 35\ndef get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n    \"\"\"\n    Args:\n        source: Tensor, shape [full_seq_len, batch_size]\n        i: int\n\n    Returns:\n        tuple (data, target), where data has shape [seq_len, batch_size] and\n        target has shape [seq_len * batch_size]\n    \"\"\"\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:29:08.785912Z","iopub.execute_input":"2022-02-24T21:29:08.786212Z","iopub.status.idle":"2022-02-24T21:29:08.796219Z","shell.execute_reply.started":"2022-02-24T21:29:08.786179Z","shell.execute_reply":"2022-02-24T21:29:08.795196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ntokens = len(vocab)  # size of vocabulary\nemsize = 200  # embedding dimension\nd_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\nnlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\nnhead = 2  # number of heads in nn.MultiheadAttention\ndropout = 0.2  # dropout probability\nmodel = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:29:42.108412Z","iopub.execute_input":"2022-02-24T21:29:42.108754Z","iopub.status.idle":"2022-02-24T21:29:42.429669Z","shell.execute_reply.started":"2022-02-24T21:29:42.108697Z","shell.execute_reply":"2022-02-24T21:29:42.428547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\nimport time\n\ncriterion = nn.CrossEntropyLoss()\nlr = 5.0  # learning rate\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\ndef train(model: nn.Module) -> None:\n    model.train()  # turn on train mode\n    total_loss = 0.\n    log_interval = 200\n    start_time = time.time()\n    src_mask = generate_square_subsequent_mask(bptt).to(device)\n\n    num_batches = len(train_data) // bptt\n    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n        data, targets = get_batch(train_data, i)\n        batch_size = data.size(0)\n        if batch_size != bptt:  # only on last batch\n            src_mask = src_mask[:batch_size, :batch_size]\n        output = model(data, src_mask)\n        loss = criterion(output.view(-1, ntokens), targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        total_loss += loss.item()\n        if batch % log_interval == 0 and batch > 0:\n            lr = scheduler.get_last_lr()[0]\n            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n            cur_loss = total_loss / log_interval\n            ppl = math.exp(cur_loss)\n            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(model: nn.Module, eval_data: Tensor) -> float:\n    model.eval()  # turn on evaluation mode\n    total_loss = 0.\n    src_mask = generate_square_subsequent_mask(bptt).to(device)\n    with torch.no_grad():\n        for i in range(0, eval_data.size(0) - 1, bptt):\n            data, targets = get_batch(eval_data, i)\n            batch_size = data.size(0)\n            if batch_size != bptt:\n                src_mask = src_mask[:batch_size, :batch_size]\n            output = model(data, src_mask)\n            output_flat = output.view(-1, ntokens)\n            total_loss += batch_size * criterion(output_flat, targets).item()\n    return total_loss / (len(eval_data) - 1)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:30:19.135753Z","iopub.execute_input":"2022-02-24T21:30:19.136135Z","iopub.status.idle":"2022-02-24T21:30:19.170823Z","shell.execute_reply.started":"2022-02-24T21:30:19.136092Z","shell.execute_reply":"2022-02-24T21:30:19.16818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_val_loss = float('inf')\nepochs = 3\nbest_model = None\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train(model)\n    val_loss = evaluate(model, val_data)\n    val_ppl = math.exp(val_loss)\n    elapsed = time.time() - epoch_start_time\n    print('-' * 89)\n    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n    print('-' * 89)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model = copy.deepcopy(model)\n\n    scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:30:31.924383Z","iopub.execute_input":"2022-02-24T21:30:31.924734Z","iopub.status.idle":"2022-02-24T21:32:30.149606Z","shell.execute_reply.started":"2022-02-24T21:30:31.924687Z","shell.execute_reply":"2022-02-24T21:32:30.148532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss = evaluate(best_model, test_data)\ntest_ppl = math.exp(test_loss)\nprint('=' * 89)\nprint(f'| End of training | test loss {test_loss:5.2f} | '\n      f'test ppl {test_ppl:8.2f}')\nprint('=' * 89)","metadata":{"execution":{"iopub.status.busy":"2022-02-24T21:33:53.476217Z","iopub.execute_input":"2022-02-24T21:33:53.476602Z","iopub.status.idle":"2022-02-24T21:33:55.668126Z","shell.execute_reply.started":"2022-02-24T21:33:53.476552Z","shell.execute_reply":"2022-02-24T21:33:55.667143Z"},"trusted":true},"execution_count":null,"outputs":[]}]}