{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install captum\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport captum\nfrom captum.attr import IntegratedGradients, Occlusion, LayerGradCam, LayerAttribution\nfrom captum.attr import visualization as viz\n\nimport os, sys\nimport json\n\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-03T22:02:06.680514Z","iopub.execute_input":"2022-02-03T22:02:06.680848Z","iopub.status.idle":"2022-02-03T22:02:18.025981Z","shell.execute_reply.started":"2022-02-03T22:02:06.68076Z","shell.execute_reply":"2022-02-03T22:02:18.025094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = models.resnet101(pretrained=True)\nmodel = model.eval()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:02:18.027667Z","iopub.execute_input":"2022-02-03T22:02:18.028041Z","iopub.status.idle":"2022-02-03T22:02:19.174083Z","shell.execute_reply.started":"2022-02-03T22:02:18.028009Z","shell.execute_reply":"2022-02-03T22:02:19.173035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_img = Image.open('/kaggle/input/images/img/cat.jpg')\ntest_img_data = np.asarray(test_img)\nplt.imshow(test_img_data)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:02:19.17964Z","iopub.execute_input":"2022-02-03T22:02:19.180501Z","iopub.status.idle":"2022-02-03T22:02:19.653766Z","shell.execute_reply.started":"2022-02-03T22:02:19.180447Z","shell.execute_reply":"2022-02-03T22:02:19.652954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model expects 224x224 3-color image\ntransform = transforms.Compose([\n transforms.Resize(224),\n transforms.CenterCrop(224),\n transforms.ToTensor()\n])\n\n# standard ImageNet normalization\ntransform_normalize = transforms.Normalize(\n     mean=[0.485, 0.456, 0.406],\n     std=[0.229, 0.224, 0.225]\n )\n\ntransformed_img = transform(test_img)\ninput_img = transform_normalize(transformed_img)\ninput_img = input_img.unsqueeze(0) # the model requires a dummy batch dimension\n\nlabels_path = '/kaggle/input/images/img/imagenet_class_index.json'\nwith open(labels_path) as json_data:\n    idx_to_labels = json.load(json_data)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:02:19.654927Z","iopub.execute_input":"2022-02-03T22:02:19.655246Z","iopub.status.idle":"2022-02-03T22:02:19.705854Z","shell.execute_reply.started":"2022-02-03T22:02:19.655217Z","shell.execute_reply":"2022-02-03T22:02:19.705163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = model(input_img)\noutput = F.softmax(output, dim=1)\nprediction_score, pred_label_idx = torch.topk(output, 1)\npred_label_idx.squeeze_()\npredicted_label = idx_to_labels[str(pred_label_idx.item())][1]\nprint('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:02:19.707655Z","iopub.execute_input":"2022-02-03T22:02:19.708037Z","iopub.status.idle":"2022-02-03T22:02:20.016276Z","shell.execute_reply.started":"2022-02-03T22:02:19.708005Z","shell.execute_reply":"2022-02-03T22:02:20.015601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the attribution algorithm with the model\nintegrated_gradients = IntegratedGradients(model)\n\n# Ask the algorithm to attribute our output target to\nattributions_ig = integrated_gradients.attribute(input_img, target=pred_label_idx, n_steps=200)\n\n# Show the original image for comparison\n_ = viz.visualize_image_attr(None, np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                      method=\"original_image\", title=\"Original Image\")\n\ndefault_cmap = LinearSegmentedColormap.from_list('custom blue',\n                                                 [(0, '#ffffff'),\n                                                  (0.25, '#0000ff'),\n                                                  (1, '#0000ff')], N=256)\n\n_ = viz.visualize_image_attr(np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1,2,0)),\n                             np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                             method='heat_map',\n                             cmap=default_cmap,\n                             show_colorbar=True,\n                             sign='positive',\n                             title='Integrated Gradients')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:03:32.657721Z","iopub.execute_input":"2022-02-03T22:03:32.658618Z","iopub.status.idle":"2022-02-03T22:04:42.337859Z","shell.execute_reply.started":"2022-02-03T22:03:32.658572Z","shell.execute_reply":"2022-02-03T22:04:42.33699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"occlusion = Occlusion(model)\n\nattributions_occ = occlusion.attribute(input_img,\n                                       target=pred_label_idx,\n                                       strides=(3, 8, 8),\n                                       sliding_window_shapes=(3,15, 15),\n                                       baselines=0)\n\n\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"heat_map\", \"heat_map\", \"masked_image\"],\n                                      [\"all\", \"positive\", \"negative\", \"positive\"],\n                                      show_colorbar=True,\n                                      titles=[\"Original\", \"Positive Attribution\", \"Negative Attribution\", \"Masked\"],\n                                      fig_size=(18, 6)\n                                     )","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:05:37.518494Z","iopub.execute_input":"2022-02-03T22:05:37.518886Z","iopub.status.idle":"2022-02-03T22:08:07.479328Z","shell.execute_reply.started":"2022-02-03T22:05:37.518846Z","shell.execute_reply":"2022-02-03T22:08:07.478436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"layer_gradcam = LayerGradCam(model, model.layer3[1].conv2)\nattributions_lgc = layer_gradcam.attribute(input_img, target=pred_label_idx)\n\n_ = viz.visualize_image_attr(attributions_lgc[0].cpu().permute(1,2,0).detach().numpy(),\n                             sign=\"all\",\n                             title=\"Layer 3 Block 1 Conv 2\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:11:00.951029Z","iopub.execute_input":"2022-02-03T22:11:00.951391Z","iopub.status.idle":"2022-02-03T22:11:01.705586Z","shell.execute_reply.started":"2022-02-03T22:11:00.951353Z","shell.execute_reply":"2022-02-03T22:11:01.704568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"upsamp_attr_lgc = LayerAttribution.interpolate(attributions_lgc, input_img.shape[2:])\n\nprint(attributions_lgc.shape)\nprint(upsamp_attr_lgc.shape)\nprint(input_img.shape)\n\n_ = viz.visualize_image_attr_multiple(upsamp_attr_lgc[0].cpu().permute(1,2,0).detach().numpy(),\n                                      transformed_img.permute(1,2,0).numpy(),\n                                      [\"original_image\",\"blended_heat_map\",\"masked_image\"],\n                                      [\"all\",\"positive\",\"positive\"],\n                                      show_colorbar=True,\n                                      titles=[\"Original\", \"Positive Attribution\", \"Masked\"],\n                                      fig_size=(18, 6))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:11:34.125405Z","iopub.execute_input":"2022-02-03T22:11:34.125706Z","iopub.status.idle":"2022-02-03T22:11:35.118445Z","shell.execute_reply.started":"2022-02-03T22:11:34.125674Z","shell.execute_reply":"2022-02-03T22:11:35.117533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imgs = ['/kaggle/input/images/img/cat.jpg', \n        '/kaggle/input/images/img/teapot.jpg', \n        '/kaggle/input/images/img/trilobite.jpg']\n\nfor img in imgs:\n    img = Image.open(img)\n    transformed_img = transform(img)\n    input_img = transform_normalize(transformed_img)\n    input_img = input_img.unsqueeze(0) # the model requires a dummy batch dimension\n\n    output = model(input_img)\n    output = F.softmax(output, dim=1)\n    prediction_score, pred_label_idx = torch.topk(output, 1)\n    pred_label_idx.squeeze_()\n    predicted_label = idx_to_labels[str(pred_label_idx.item())][1]\n    print('Predicted:', predicted_label, '/', pred_label_idx.item(), ' (', prediction_score.squeeze().item(), ')')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:14:12.515592Z","iopub.execute_input":"2022-02-03T22:14:12.516178Z","iopub.status.idle":"2022-02-03T22:14:13.584223Z","shell.execute_reply.started":"2022-02-03T22:14:12.516138Z","shell.execute_reply":"2022-02-03T22:14:13.583317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from captum.insights import AttributionVisualizer, Batch\nfrom captum.insights.attr_vis.features import ImageFeature\n\n# Baseline is all-zeros input - this may differ depending on your data\ndef baseline_func(input):\n    return input * 0\n\n# merging our image transforms from above\ndef full_img_transform(input):\n    i = Image.open(input)\n    i = transform(i)\n    i = transform_normalize(i)\n    i = i.unsqueeze(0)\n    return i\n\n\ninput_imgs = torch.cat(list(map(lambda i: full_img_transform(i), imgs)), 0)\n\nvisualizer = AttributionVisualizer(\n    models=[model],\n    score_func=lambda o: torch.nn.functional.softmax(o, 1),\n    classes=list(map(lambda k: idx_to_labels[k][1], idx_to_labels.keys())),\n    features=[\n        ImageFeature(\n            \"Photo\",\n            baseline_transforms=[baseline_func],\n            input_transforms=[],\n        )\n    ],\n    dataset=[Batch(input_imgs, labels=[282,849,69])]\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T22:15:02.323476Z","iopub.execute_input":"2022-02-03T22:15:02.323856Z","iopub.status.idle":"2022-02-03T22:15:02.803873Z","shell.execute_reply.started":"2022-02-03T22:15:02.323821Z","shell.execute_reply":"2022-02-03T22:15:02.802577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualizer.render()","metadata":{},"execution_count":null,"outputs":[]}]}