{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# PyTorch model and training necessities\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n# Image datasets and image manipulation\nimport torchvision\nimport torchvision.transforms as transforms\n\n# Image display\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# PyTorch TensorBoard support\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-03T21:26:20.247109Z","iopub.execute_input":"2022-02-03T21:26:20.248068Z","iopub.status.idle":"2022-02-03T21:26:22.493265Z","shell.execute_reply.started":"2022-02-03T21:26:20.247847Z","shell.execute_reply":"2022-02-03T21:26:22.491984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Gather datasets and prepare them for consumption\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))])\n\n# Store separate training and validations splits in ./data\ntraining_set = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=True,\n    transform=transform)\nvalidation_set = torchvision.datasets.FashionMNIST('./data',\n    download=True,\n    train=False,\n    transform=transform)\n\ntraining_loader = torch.utils.data.DataLoader(training_set,\n                                              batch_size=4,\n                                              shuffle=True,\n                                              num_workers=2)\n\n\nvalidation_loader = torch.utils.data.DataLoader(validation_set,\n                                                batch_size=4,\n                                                shuffle=False,\n                                                num_workers=2)\n\n# Class labels\nclasses = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n\n# Helper function for inline image display\ndef matplotlib_imshow(img, one_channel=False):\n    if one_channel:\n        img = img.mean(dim=0)\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    if one_channel:\n        plt.imshow(npimg, cmap=\"Greys\")\n    else:\n        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n# Extract a batch of 4 images\ndataiter = iter(training_loader)\nimages, labels = dataiter.next()\n\n# Create a grid from the images and show them\nimg_grid = torchvision.utils.make_grid(images)\nmatplotlib_imshow(img_grid, one_channel=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T21:27:27.060206Z","iopub.execute_input":"2022-02-03T21:27:27.060522Z","iopub.status.idle":"2022-02-03T21:27:32.840229Z","shell.execute_reply.started":"2022-02-03T21:27:27.060491Z","shell.execute_reply":"2022-02-03T21:27:32.839048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Default log_dir argument is \"runs\" - but it's good to be specific\n# torch.utils.tensorboard.SummaryWriter is imported above\nwriter = SummaryWriter('runs/fashion_mnist_experiment_1')\n\n# Write image data to TensorBoard log dir\nwriter.add_image('Four Fashion-MNIST Images', img_grid)\nwriter.flush()\n\n# To view, start TensorBoard on the command line with:\n#   tensorboard --logdir=runs\n# ...and open a browser tab to http://localhost:6006/","metadata":{"execution":{"iopub.status.busy":"2022-02-03T21:28:31.507317Z","iopub.execute_input":"2022-02-03T21:28:31.507654Z","iopub.status.idle":"2022-02-03T21:28:37.156119Z","shell.execute_reply.started":"2022-02-03T21:28:31.50762Z","shell.execute_reply":"2022-02-03T21:28:37.155377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 4 * 4)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T21:30:25.911999Z","iopub.execute_input":"2022-02-03T21:30:25.912793Z","iopub.status.idle":"2022-02-03T21:30:25.931632Z","shell.execute_reply.started":"2022-02-03T21:30:25.912744Z","shell.execute_reply":"2022-02-03T21:30:25.930794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(validation_loader))\nfor epoch in range(1):  # loop over the dataset multiple times\n    running_loss = 0.0\n\n    for i, data in enumerate(training_loader, 0):\n        # basic training loop\n        inputs, labels = data\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 1000 == 999:    # Every 1000 mini-batches...\n            print('Batch {}'.format(i + 1))\n            # Check against the validation set\n            running_vloss = 0.0\n\n            net.train(False) # Don't need to track gradents for validation\n            for j, vdata in enumerate(validation_loader, 0):\n                vinputs, vlabels = vdata\n                voutputs = net(vinputs)\n                vloss = criterion(voutputs, vlabels)\n                running_vloss += vloss.item()\n            net.train(True) # Turn gradients back on for training\n\n            avg_loss = running_loss / 1000\n            avg_vloss = running_vloss / len(validation_loader)\n\n            # Log the running loss averaged per batch\n            writer.add_scalars('Training vs. Validation Loss',\n                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n                            epoch * len(training_loader) + i)\n\n            running_loss = 0.0\nprint('Finished Training')\n\nwriter.flush()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T21:31:35.763402Z","iopub.execute_input":"2022-02-03T21:31:35.76369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Again, grab a single mini-batch of images\ndataiter = iter(training_loader)\nimages, labels = dataiter.next()\n\n# add_graph() will trace the sample input through your model,\n# and render it as a graph.\nwriter.add_graph(net, images)\nwriter.flush()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select a random subset of data and corresponding labels\ndef select_n_random(data, labels, n=100):\n    assert len(data) == len(labels)\n\n    perm = torch.randperm(len(data))\n    return data[perm][:n], labels[perm][:n]\n\n# Extract a random subset of data\nimages, labels = select_n_random(training_set.data, training_set.targets)\n\n# get the class labels for each image\nclass_labels = [classes[label] for label in labels]\n\n# log embeddings\nfeatures = images.view(-1, 28 * 28)\nwriter.add_embedding(features,\n                    metadata=class_labels,\n                    label_img=images.unsqueeze(1))\nwriter.flush()\nwriter.close()","metadata":{},"execution_count":null,"outputs":[]}]}