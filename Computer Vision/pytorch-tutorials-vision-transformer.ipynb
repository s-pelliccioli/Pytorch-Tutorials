{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install timm\n\nfrom PIL import Image\nimport torch\nimport timm\nimport requests\nimport torchvision.transforms as transforms\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nprint(torch.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-12T21:57:19.056791Z","iopub.execute_input":"2022-02-12T21:57:19.057065Z","iopub.status.idle":"2022-02-12T21:57:29.957455Z","shell.execute_reply.started":"2022-02-12T21:57:19.057034Z","shell.execute_reply":"2022-02-12T21:57:29.956657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.Resize(256, interpolation=3),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n])\n\nimg = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\nimg = transform(img)[None,]\nout = model(img)\nclsidx = torch.argmax(out)\nprint(clsidx.item())","metadata":{"execution":{"iopub.status.busy":"2022-02-12T21:58:22.267775Z","iopub.execute_input":"2022-02-12T21:58:22.268069Z","iopub.status.idle":"2022-02-12T21:58:31.920432Z","shell.execute_reply.started":"2022-02-12T21:58:22.268034Z","shell.execute_reply":"2022-02-12T21:58:31.919676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\nmodel.eval()\nscripted_model = torch.jit.script(model)\nscripted_model.save(\"fbdeit_scripted.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T21:59:29.994141Z","iopub.execute_input":"2022-02-12T21:59:29.994644Z","iopub.status.idle":"2022-02-12T21:59:32.661375Z","shell.execute_reply.started":"2022-02-12T21:59:29.994604Z","shell.execute_reply":"2022-02-12T21:59:32.660546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Use 'fbgemm' for server inference and 'qnnpack' for mobile inference\nbackend = \"fbgemm\" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook\nmodel.qconfig = torch.quantization.get_default_qconfig(backend)\ntorch.backends.quantized.engine = backend\n\nquantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\nscripted_quantized_model = torch.jit.script(quantized_model)\nscripted_quantized_model.save(\"fbdeit_scripted_quantized.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T22:00:42.639307Z","iopub.execute_input":"2022-02-12T22:00:42.639572Z","iopub.status.idle":"2022-02-12T22:00:45.232314Z","shell.execute_reply.started":"2022-02-12T22:00:42.639543Z","shell.execute_reply":"2022-02-12T22:00:45.231547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = scripted_quantized_model(img)\nclsidx = torch.argmax(out)\nprint(clsidx.item())\n# The same output 269 should be printed","metadata":{"execution":{"iopub.status.busy":"2022-02-12T22:02:55.959479Z","iopub.execute_input":"2022-02-12T22:02:55.960231Z","iopub.status.idle":"2022-02-12T22:02:56.615805Z","shell.execute_reply.started":"2022-02-12T22:02:55.960189Z","shell.execute_reply":"2022-02-12T22:02:56.615038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.mobile_optimizer import optimize_for_mobile\noptimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\noptimized_scripted_quantized_model.save(\"fbdeit_optimized_scripted_quantized.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T22:03:24.796496Z","iopub.execute_input":"2022-02-12T22:03:24.796809Z","iopub.status.idle":"2022-02-12T22:03:26.735731Z","shell.execute_reply.started":"2022-02-12T22:03:24.796774Z","shell.execute_reply":"2022-02-12T22:03:26.735006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out = optimized_scripted_quantized_model(img)\nclsidx = torch.argmax(out)\nprint(clsidx.item())\n# Again, the same output 269 should be printed","metadata":{"execution":{"iopub.status.busy":"2022-02-12T22:03:55.499675Z","iopub.execute_input":"2022-02-12T22:03:55.499997Z","iopub.status.idle":"2022-02-12T22:03:55.848218Z","shell.execute_reply.started":"2022-02-12T22:03:55.499942Z","shell.execute_reply":"2022-02-12T22:03:55.8457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\nptl = torch.jit.load(\"fbdeit_optimized_scripted_quantized_lite.ptl\")","metadata":{"execution":{"iopub.status.busy":"2022-02-12T22:04:19.463244Z","iopub.execute_input":"2022-02-12T22:04:19.463496Z","iopub.status.idle":"2022-02-12T22:04:20.668537Z","shell.execute_reply.started":"2022-02-12T22:04:19.463467Z","shell.execute_reply":"2022-02-12T22:04:20.667747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.autograd.profiler.profile(use_cuda=False) as prof1:\n    out = model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof2:\n    out = scripted_model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof3:\n    out = scripted_quantized_model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof4:\n    out = optimized_scripted_quantized_model(img)\nwith torch.autograd.profiler.profile(use_cuda=False) as prof5:\n    out = ptl(img)\n\nprint(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\nprint(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\nprint(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\nprint(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\nprint(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))","metadata":{"execution":{"iopub.status.busy":"2022-02-12T22:05:44.638513Z","iopub.execute_input":"2022-02-12T22:05:44.638787Z","iopub.status.idle":"2022-02-12T22:05:48.379768Z","shell.execute_reply.started":"2022-02-12T22:05:44.638757Z","shell.execute_reply":"2022-02-12T22:05:48.378371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\ndf = pd.concat([df, pd.DataFrame([\n    [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n    [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n    [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n    [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n    [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],\n    columns=['Inference Time', 'Reduction'])], axis=1)\n\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-12T22:06:31.292811Z","iopub.execute_input":"2022-02-12T22:06:31.293538Z","iopub.status.idle":"2022-02-12T22:06:31.571329Z","shell.execute_reply.started":"2022-02-12T22:06:31.293485Z","shell.execute_reply":"2022-02-12T22:06:31.570542Z"},"trusted":true},"execution_count":null,"outputs":[]}]}