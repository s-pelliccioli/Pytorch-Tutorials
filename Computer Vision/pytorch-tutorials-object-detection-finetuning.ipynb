{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install cython\n# Install pycocotools, the version by default in Colab\n# has a bug fixed in https://github.com/cocodataset/cocoapi/pull/354\n!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-08T21:12:14.521374Z","iopub.execute_input":"2022-02-08T21:12:14.521742Z","iopub.status.idle":"2022-02-08T21:12:47.537171Z","shell.execute_reply.started":"2022-02-08T21:12:14.521654Z","shell.execute_reply":"2022-02-08T21:12:47.53613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nImage.open('/kaggle/input/pennfudan/PennFudanPed/PNGImages/FudanPed00001.png')","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:15:22.28587Z","iopub.execute_input":"2022-02-08T21:15:22.286477Z","iopub.status.idle":"2022-02-08T21:15:22.485217Z","shell.execute_reply.started":"2022-02-08T21:15:22.286415Z","shell.execute_reply":"2022-02-08T21:15:22.484391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = Image.open('/kaggle/input/pennfudan/PennFudanPed/PedMasks/FudanPed00001_mask.png')\n# each mask instance has a different color, from zero to N, where\n# N is the number of instances. In order to make visualization easier,\n# let's adda color palette to the mask.\nmask.putpalette([\n    0, 0, 0, # black background\n    255, 0, 0, # index 1 is red\n    255, 255, 0, # index 2 is yellow\n    255, 153, 0, # index 3 is orange\n])\nmask","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:16:33.915507Z","iopub.execute_input":"2022-02-08T21:16:33.915794Z","iopub.status.idle":"2022-02-08T21:16:33.938619Z","shell.execute_reply.started":"2022-02-08T21:16:33.915764Z","shell.execute_reply":"2022-02-08T21:16:33.937594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.utils.data\nfrom PIL import Image\n\n\nclass PennFudanDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transforms=None):\n        self.root = root\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images ad masks\n        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n        img = Image.open(img_path).convert(\"RGB\")\n        # note that we haven't converted the mask to RGB,\n        # because each color corresponds to a different instance\n        # with 0 being background\n        mask = Image.open(mask_path)\n\n        mask = np.array(mask)\n        # instances are encoded as different colors\n        obj_ids = np.unique(mask)\n        # first id is the background, so remove it\n        obj_ids = obj_ids[1:]\n\n        # split the color-encoded mask into a set\n        # of binary masks\n        masks = mask == obj_ids[:, None, None]\n\n        # get bounding box coordinates for each mask\n        num_objs = len(obj_ids)\n        boxes = []\n        for i in range(num_objs):\n            pos = np.where(masks[i])\n            xmin = np.min(pos[1])\n            xmax = np.max(pos[1])\n            ymin = np.min(pos[0])\n            ymax = np.max(pos[0])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        # there is only one class\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:17:30.645306Z","iopub.execute_input":"2022-02-08T21:17:30.645658Z","iopub.status.idle":"2022-02-08T21:17:32.152773Z","shell.execute_reply.started":"2022-02-08T21:17:30.645629Z","shell.execute_reply":"2022-02-08T21:17:32.151743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finetuning","metadata":{}},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (person) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modify backbone","metadata":{}},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# load a pre-trained model for classification and return\n# only the features\nbackbone = torchvision.models.mobilenet_v2(pretrained=True).features\n# FasterRCNN needs to know the number of\n# output channels in a backbone. For mobilenet_v2, it's 1280\n# so we need to add it here\nbackbone.out_channels = 1280\n\n# let's make the RPN generate 5 x 3 anchors per spatial\n# location, with 5 different sizes and 3 different aspect\n# ratios. We have a Tuple[Tuple[int]] because each feature\n# map could potentially have different sizes and\n# aspect ratios \nanchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n\n# let's define what are the feature maps that we will\n# use to perform the region of interest cropping, as well as\n# the size of the crop after rescaling.\n# if your backbone returns a Tensor, featmap_names is expected to\n# be [0]. More generally, the backbone should return an\n# OrderedDict[Tensor], and in featmap_names you can choose which\n# feature maps to use.\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n                                                output_size=7,\n                                                sampling_ratio=2)\n\n# put the pieces together inside a FasterRCNN model\nmodel = FasterRCNN(backbone,\n                   num_classes=2,\n                   rpn_anchor_generator=anchor_generator,\n                   box_roi_pool=roi_pooler)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n\n      \ndef get_instance_segmentation_model(num_classes):\n    # load an instance segmentation model pre-trained on COCO\n    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n                                                       hidden_layer,\n                                                       num_classes)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:22:04.10759Z","iopub.execute_input":"2022-02-08T21:22:04.107939Z","iopub.status.idle":"2022-02-08T21:22:04.319186Z","shell.execute_reply.started":"2022-02-08T21:22:04.107879Z","shell.execute_reply":"2022-02-08T21:22:04.318163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download TorchVision repo to use some files from\n# references/detection\n!git clone https://github.com/pytorch/vision.git\n!cd vision\n!git checkout v0.8.2\n\n!cp references/detection/utils.py ../\n!cp references/detection/transforms.py ../\n!cp references/detection/coco_eval.py ../\n!cp references/detection/engine.py ../\n!cp references/detection/coco_utils.py ../","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:22:37.66165Z","iopub.execute_input":"2022-02-08T21:22:37.661998Z","iopub.status.idle":"2022-02-08T21:23:07.508817Z","shell.execute_reply.started":"2022-02-08T21:22:37.661939Z","shell.execute_reply":"2022-02-08T21:23:07.507722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vision.references.detection.engine import train_one_epoch, evaluate\nimport vision.references.detection.utils\nimport vision.references.detection.transforms as T\n\n\ndef get_transform(train):\n    transforms = []\n    # converts the image, a PIL image, into a PyTorch Tensor\n    transforms.append(T.ToTensor())\n    if train:\n        # during training, randomly flip the training images\n        # and ground-truth for data augmentation\n        transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2022-02-08T21:26:19.727954Z","iopub.execute_input":"2022-02-08T21:26:19.728329Z","iopub.status.idle":"2022-02-08T21:26:19.754012Z","shell.execute_reply.started":"2022-02-08T21:26:19.728271Z","shell.execute_reply":"2022-02-08T21:26:19.752587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn\n)\n# For Training\nimages,targets = next(iter(data_loader))\nimages = list(image for image in images)\ntargets = [{k: v for k, v in t.items()} for t in targets]\noutput = model(images,targets)   # Returns losses and detections\n# For inference\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)           # Returns predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use our dataset and defined transformations\ndataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\ndataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n\n# split the dataset in train and test set\ntorch.manual_seed(1)\nindices = torch.randperm(len(dataset)).tolist()\ndataset = torch.utils.data.Subset(dataset, indices[:-50])\ndataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n\n# define training and validation data loaders\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=2, shuffle=True, num_workers=4,\n    collate_fn=utils.collate_fn)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n    collate_fn=utils.collate_fn)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n# our dataset has two classes only - background and person\nnum_classes = 2\n\n# get the model using our helper function\nmodel = get_instance_segmentation_model(num_classes)\n# move model to the right device\nmodel.to(device)\n\n# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005,\n                            momentum=0.9, weight_decay=0.0005)\n\n# and a learning rate scheduler which decreases the learning rate by\n# 10x every 3 epochs\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=3,\n                                               gamma=0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's train it for 10 epochs\nfrom torch.optim.lr_scheduler import StepLR\nnum_epochs = 10\n\nfor epoch in range(num_epochs):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n    # update the learning rate\n    lr_scheduler.step()\n    # evaluate on the test dataset\n    evaluate(model, data_loader_test, device=device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pick one image from the test set\nimg, _ = dataset_test[0]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())","metadata":{},"execution_count":null,"outputs":[]}]}